{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_DIR,'data')\n",
    "IMG_DIR = os.path.join(PROJECT_ROOT_DIR, 'img')\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "    \n",
    "def display_normal(data, xlab, ylab, anomaly = None):\n",
    "    mu = data.mean()\n",
    "    variance = data.std()\n",
    "    sigma = np.sqrt(variance)\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,stats.norm.pdf(x, mu, sigma))\n",
    "    ax.scatter(data,[0]*data.shape[0], color='red', marker='x')\n",
    "    if anomaly:\n",
    "        ax.scatter(anomaly, 0, color='green', marker='x')\n",
    "    \n",
    "    ax.set(xlabel=xlab, ylabel=ylab)\n",
    "    return fig, ax\n",
    "\n",
    "def display_scatter(data, xlab, ylab, anomaly=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(heat, vibrations, color='red')\n",
    "    if anomaly:\n",
    "        if isinstance(anomaly, int):\n",
    "            ax.scatter(data[anomaly][0], data[anomaly][1], color='green', marker='x')\n",
    "        else:\n",
    "            ax.scatter(anomaly[0], anomaly[1], color='green', marker='x')\n",
    "    _ = ax.set(xlim=(0,8),ylim=(0,10), xlabel=xlab, ylabel=ylab)\n",
    "    return fig, ax\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def display_multivariate(location, sigma, range_x1 = [-5, 5], range_x2 = [-5, 5]):\n",
    "    #Create grid and multivariate normal\n",
    "    x = np.linspace(*range_x1, 500)\n",
    "    y = np.linspace(*range_x2, 500)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X; pos[:, :, 1] = Y                             \n",
    "    rv = multivariate_normal(location, sigma)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\n",
    "    ax.set_xlabel(r'X_1 axis')\n",
    "    ax.set_ylabel(r'X_2 axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    \n",
    "    return fig, ax\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "Imagine your are working in the context of aircraft manifacturing and you\n",
    "are responsible for the engine of each aircraft.\n",
    "\n",
    "Any engine is represented by the a set of features:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "x_1 & = & \\text{heat generated} \\\\\n",
    "x_2 & = & \\text{vibration intensity} \\\\\n",
    "\\dots & & \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You also have a dataset: $D = \\{x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\}$.\n",
    "\n",
    "Based on the above features, you want to be able to detect defective engines,\n",
    "i.e., anomalies.\n",
    "\n",
    "The figure below shows what at an anomaly looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = np.random.normal(5, 1, size=50).tolist() + [12]\n",
    "vibrations = np.random.normal(5, 1, size=50).tolist() + [0]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(heat[:-1], vibrations[:-1], color='red')\n",
    "ax.scatter(heat[-1], vibrations[-1], color='green', marker='x')\n",
    "ax.annotate(\"Anomaly\",\n",
    "              xy=(heat[-1], vibrations[-1]+0.2),  \n",
    "              xytext=(heat[-1]-2, vibrations[-1]+2),\n",
    "              arrowprops={\"facecolor\":'black'},\n",
    "              horizontalalignment='left',\n",
    "              verticalalignment='top' )\n",
    "_ = ax.set(xlabel=\"Heat\", ylabel=\"Vibrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a new engine $x_{test}$, how can we decide if it should be\n",
    "regarded as an anomaly or not? \n",
    "\n",
    "- We can build probabilistic model from our data. \n",
    "  Then, we can use the probability density function learned from the data \n",
    "  to classify a new sample as an anomaly.\n",
    "  \n",
    "For instance, we can establish the following rule:\n",
    "- Given a certain threshold $\\epsilon$\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\text{(i)} & \\text{if} & p(x_{test}) < \\epsilon & x_{test}\\text{ is an anomaly}\\\\\n",
    "\\text{(ii)} & \\text{if} & p(x_{test}) \\geq \\epsilon & x_{test}\\text{ is NOT an anomaly}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "## The Gaussian Distribution\n",
    "The notion of anomaly is related to the concept of _deviation_ from \n",
    "a standard distribution.\n",
    "\n",
    "Let's revise some fundamental concepts about the gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "variance = 1\n",
    "sigma = np.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "ax.bar(0, 0.5, width=.1, tick_label=r\"$\\mu$\", color=\"red\")\n",
    "ax.bar(0, 0.5, width=.1, tick_label=r\"$\\mu$\", color=\"red\")\n",
    "plt.title(\"Gaussian Distribution\")\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\")\n",
    "\n",
    "\n",
    "ax.annotate(r\"$\\sigma$\",\n",
    "              xy=(0, 0.3),  \n",
    "              xytext=(1, 0.3),\n",
    "              arrowprops={\"facecolor\":'red', \"arrowstyle\":'<->'},\n",
    "              horizontalalignment='right',\n",
    "              verticalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal distributed variable $x$ is denoted by $x \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where:\n",
    "- $\\mu$ denotes the mean\n",
    "- $\\sigma$ denotes is the standard deviation - $\\sigma^2$ is the variance.\n",
    "\n",
    "The density function is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "p(x; \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\big(-\\frac{(x-\\mu)^2}{2\\sigma^2} \\big)\n",
    "$$\n",
    "\n",
    "Intuitively: \n",
    "- larger values of $\\sigma$ make the shape of the curve wider and shorter\n",
    "- smaller values of $\\sigma$ make the shape of the curve thinner and taller \n",
    "- the  value of $\\mu$ shifts the entire curve on the x-axis.\n",
    "\n",
    "\n",
    "### Fitting the normal distribution - Maximum Likelihood Estimation (MLE)\n",
    "We need to estimate both the values of $\\mu$ and $\\sigma$. \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mu & = & \\frac{1}{m} \\sum_{i} x^{(i)}\\\\\n",
    "\\sigma^2 & = & \\frac{1}{m} \\sum_{i} (x^{(i)} - \\mu)^2 \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The above estimation is based on a single important assumption: data are distributed according\n",
    "to a normal distribution (__You need to verify this assumption__).\n",
    "\n",
    "## Anomaly Detection Algorithm\n",
    "Given a training set $D = \\{x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\}$, __assuming each \n",
    "feature i is distributed according to its own normal distribution with parameters\n",
    "$\\mu_i$ and $\\sigma_i$__ We can compute the probability of a sample $x \\in \\mathbf{R}^N$ as:\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{i}p(x_i; \\mu_i,  \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "The above product defines a probability distribution which is also a Gaussian.\n",
    "\n",
    "\n",
    "### Putting all together \n",
    "1. Choose features $x_i$ that might be indicative for an anomalous example. \n",
    "2. Fit the parameters with MLE \n",
    "3. Label a new example x as an anomaly if:\n",
    "$$\n",
    "p(x) = \\prod_{i}p(x_i; \\mu_i,  \\sigma_i^2) < \\epsilon\n",
    "$$\n",
    "\n",
    "__Note__: The entire algorithm assumes the features are independent from one another.\n",
    "\n",
    "Nonetheless, the approach works sufficiently well even though the above assumption does not hold.\n",
    "\n",
    "\n",
    "### Performance Analysis\n",
    "We are interested in finding a single metric of evaluation.\n",
    "\n",
    "With a little of imagination we can reduce the anomaly detection problem to a classification\n",
    "problem. More specifically, it is similar to a binary classification problem, where the target\n",
    "variable is 1 whenever a sample is regarded as an anomaly.\n",
    "\n",
    "A typical scenario is the following: \n",
    "- Training Set: a set of unlabeled data. This samples are used to MLE, thus to reconstruct the \n",
    "  probability distribution of the so called _normal examples_\n",
    "- Validation and Test Set. These are collections of labeled data of the form $(x_{cv},y_{cv})$ and\n",
    "  $(x_{test}, y_{test})$  for the validation and the test set respectively.\n",
    "  \n",
    "  \n",
    "  \n",
    "While the training set has to contain the samples regarded as normal, the last two datasets \n",
    "have both a combination normal samples and anomalies. \n",
    "\n",
    "Usually, anomalies are far less than the number of normal samples. \n",
    "A typical scenario would be the following: \n",
    "- Original Data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "10.000 & - & \\text{good engines} \\\\\n",
    "20 & - & \\text{flawed engines} \n",
    "\\end{array}\n",
    "$$\n",
    "splitted into: \n",
    "\n",
    "| Segment        | Normal (y=0) | Flawed (y=1) |\n",
    "|:--------------:|:------------:|:------------:|\n",
    "| Training Set   | 6000         | 0            |\n",
    "| Validation Set | 2000         | 10           |\n",
    "| Test Set       | 2000         | 10           |\n",
    "\n",
    "\n",
    "Given the above splitting, the evaluation is carried out as follow:\n",
    "\n",
    "1. Fit the model to estimate $p(x)$ on the __Training Set__\n",
    "2. Choose e value for $\\epsilon$ and classify all the points in the __Validation Set__\n",
    "3. Evaluate the quality of the classification according to either one of the following metrics:\n",
    "   1. Accuracy (really?)\n",
    "   2. Precision\n",
    "   3. Recall\n",
    "   4. F1-Score\n",
    "4. Return to step 2 and try with a different value of $\\epsilon$.\n",
    "\n",
    "At the end of step 4, after you have established the best value of \n",
    "$\\epsilon$ with respect to the performace reported on the validation set, \n",
    "compute quality of your solution on the  __Test Set__.\n",
    "\n",
    "### Recap\n",
    "Summarize the main differences and analogies between the \n",
    "anomaly detection problem and the classification problem:\n",
    "\n",
    " | Anomaly Detection | Classification |\n",
    " |-------------------|----------------|\n",
    " |There is a small number of \"positive\" samples                   |      Data are more balanced          |\n",
    " |Requires few \"positive\" sample                   |                |\n",
    " \n",
    "### Tips and tricks\n",
    "One of the key assumption of the above approach is that data needs to be\n",
    "distributed according to some normal distribution. \n",
    "\n",
    "Therefore, it is of paramount importance  to check this assumption before applying \n",
    "the algorithm. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(5, .5, size=100).tolist() \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data, density=True, )\n",
    "sns.kdeplot(data)\n",
    "plt.title(\"Gaussian Distribution\")\n",
    "ax.text(5.5, 0.6, \"This is ok!\")\n",
    "_ = ax.set(xlabel=\"x\", ylabel=\"p(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.exponential(1, 100) \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data, density=True)\n",
    "plt.title(\"Exp.  Distribution\")\n",
    "ax.text(3, 0.5, \"This is not ok!\")\n",
    "_ = ax.set(xlabel=\"x\", ylabel=\"p(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is required a little effort to enforce a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(np.log(data+c), density=True)\n",
    "sns.kdeplot(data)\n",
    "plt.title(\"(Almost) Gaussian  Distribution\")\n",
    "ax.text(3, 0.5, \"This is quite good!\")\n",
    "_ = ax.set(xlabel=\"x\", ylabel=\"p(x)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data ** 1/c, density=True)\n",
    "sns.kdeplot(data)\n",
    "\n",
    "plt.title(\"(Almost) Gaussian  Distribution\")\n",
    "ax.text(3, 0.5, \"This is quite good!\")\n",
    "_ = ax.set(xlabel=\"x\", ylabel=\"p(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature selection is not an easy task. \n",
    "There are two main strategies:\n",
    "1. Looking at the attributes distribution independently from one another\n",
    "2. Looking at the joint distribution of two or more variables. \n",
    "\n",
    "Clearly, the first strategy is very simple. \n",
    "\n",
    "However, it is not able to detect the following situation.\n",
    "\n",
    "Imagine you already know there is a flawed engine. \n",
    "\n",
    "Let's see what happens when you plot your data\n",
    "separately.\n",
    "\n",
    "__Heat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = [5, 5.1] #\n",
    "heat = np.random.normal(5, 1, size=50)\n",
    "vibrations = np.random.normal(7, 1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = heat.mean()\n",
    "variance = heat.std()\n",
    "sigma = np.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,stats.norm.pdf(x, mu, sigma))\n",
    "ax.scatter(heat,[0]*heat.shape[0], color='red', marker='x')\n",
    "ax.scatter(anomaly[0],0, color='green', marker='x')\n",
    "_ = ax.set(xlabel=\"Heat\", ylabel=\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = vibrations.mean()\n",
    "variance = vibrations.std()\n",
    "sigma = np.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,stats.norm.pdf(x, mu, sigma))\n",
    "ax.scatter(vibrations,[0]*heat.shape[0], color='red', marker='x')\n",
    "ax.scatter(anomaly[1],0, color='green', marker='x')\n",
    "_ = ax.set(xlabel=\"Vibrations\", ylabel=\"P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vibration__ vs __Heat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(heat, vibrations, color='red')\n",
    "ax.scatter(anomaly[0], anomaly[1], vibrations[-1], color='green', marker='x')\n",
    "_ = ax.set(xlim=(0,8),ylim=(0,10), xlabel=\"Heat\", ylabel=\"Vibrations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure is clear that the green point represents an anomaly.\n",
    "\n",
    "__Traps to avoid__\n",
    "\n",
    "A common problem when determining the proper set of features\n",
    "to train the model is when anomalous and normal sample have\n",
    "comparable probabilities.\n",
    "\n",
    "The probability of the anomalous sample is given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heat_mu, heat_sigma = heat.mean(), np.sqrt(heat.std())\n",
    "vib_mu, vib_sigma = vibrations.mean(), np.sqrt(vibrations.std())\n",
    "\n",
    "def p(x, mu, sigma):\n",
    "    power = -.5 *((x-mu)/sigma)**2\n",
    "    return 1/(sigma * np.sqrt(2*np.pi))*np.exp(power)\n",
    "\n",
    "anomaly_p = p(anomaly[0], heat_mu, heat_sigma)*p(anomaly[1], vib_mu, vib_sigma)\n",
    "normal_p = p(heat, heat_mu, heat_sigma) * p(vibrations, vib_mu, vib_sigma)\n",
    "\n",
    "\n",
    "_ = plt.hist(anomaly_p-normal_p, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the probability of the anomaly, we notice that it is inconveniently\n",
    "close with any other normal sample. \n",
    "\n",
    "__How can we solve this problem?__\n",
    "You can come up with new features.\n",
    "\n",
    "For instance, what happens if we divide the create a new feature\n",
    "as follows:\n",
    "\n",
    "$$\n",
    "x_3 = \\frac{\\text{heat}}{\\text{vibrations}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = heat/(vibrations**2)\n",
    "\n",
    "_ = display_normal(x3, r\"Heat/Vibrations$^2$\", \"p(x)\", anomaly[0]/anomaly[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature allows us to immediately spot the flawed engine!\n",
    "\n",
    "## Multivariate Gaussian\n",
    "\n",
    "A major weakness of the above strategy is the inability to reason about a bivariate \n",
    "perspective. \n",
    "\n",
    "Let's see the following example. \n",
    "\n",
    "__Monitoring Machines in a data center__\n",
    "\n",
    "We want to detect anomalies based on the amount of CPU and memory used by a\n",
    "process. \n",
    "\n",
    "We might have the followng situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = np.sort(np.random.normal(5, 2, size=50))\n",
    "memory = np.sort(np.random.normal(5,2 , size=50))\n",
    "anomaly = [3.2, 7]\n",
    "data = np.c_[cpu, memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_normal(cpu, \"CPU\", \"p(x)\", anomaly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = display_normal(memory, \"Memory\", \"p(x)\", anomaly[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_scatter(data, \"cpu\", \"memory\", anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot appears to suggest that memory increases with the cpu load. \n",
    "\n",
    "Clearly, the green point represents an anomaly, as it couples a low cpu load with \n",
    "a high memory sonsumption.\n",
    "\n",
    "Nonetheless, if we had not plot the bivariate scatter plot, we would not \n",
    "be able to detect this anomaly. \n",
    "\n",
    "The main problem with the algorithm discussed in the previous section is that \n",
    "it is unable to adopt a bivariate perspective -- __this is due to the assumptions that variables/atributes are independent from each others!__\n",
    "\n",
    "Using a multivariate normal distribution, the probability of a sample $p(x)$ is not considered as \n",
    "the product of independent normally distributed variables, but $x$ becomes a multivariate variable.\n",
    "\n",
    "Therefore we need a new set of parameters:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\mu \\in \\mathbf{R}^n & \\text{location} \\\\\n",
    "\\Sigma \\in \\mathbf{R}^{n \\times n} & \\text{covariance matrix} \n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "A multivariate normal distribution is then defined as:\n",
    "\n",
    "$$\n",
    "p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2} det(\\Sigma)^{1/2}} \\,\\exp\\big(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\big)\n",
    "$$\n",
    "\n",
    "### What a multivariate normal distribution looks like? \n",
    "We need to define the required parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "location = [1, 0]\n",
    "\n",
    "sigma = [[2.0, 0], \n",
    "         [0, 2.0]]\n",
    "\n",
    "fig, ax = display_multivariate(location, sigma)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions__\n",
    "\n",
    "1. What happen when you change the location vector?\n",
    "2. What happen when you change the element on the main diagonal of the covariance matrix?\n",
    "3. What happen when you change the element not on the main diagonal of the covariance matrix?\n",
    "4. What is the relation between the multivariate approach and the previous one?\n",
    "   Hint: Focus on the covariance matrix. \n",
    "   \n",
    "   \n",
    "# Original Model vs Multivariate \n",
    "\n",
    "| Question       | Original Model |  Multivariate|\n",
    "|:--------------:|:------------:|:------------:|\n",
    "| requires manual feat. eng.   |     |    |\n",
    "| easy to train                |     |    |\n",
    "| requires a lot of data       |     |    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "anomaly-detection.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
